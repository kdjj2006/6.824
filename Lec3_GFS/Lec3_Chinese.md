6.824 2018 第3课：GFS

The Google File System  
Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung     
SOSP 2003   

* 我们为什么要读这篇论文？
    * 用于 map/reduce 的文件系统
    * 6.284 课程的主要主题都在文中有所展示
        * 简单性和性能的交易一致性
        * 后续设计的动机
    * 很好的系统论文 -- 从应用到网络的详细信息
        * 性能，容错，一致性
    * 有影响力
        * 很多其他的系统使用 GFS(例如 Bigtable, Spanner)
        * HDFS (Hadoop Distributed File System) 基于 GFS
* 什么是一致性
    * 一个正确的条件
    * 在数据是多份时很重要但是很难实现
        * 特别是当应用并发访问的时候
        * 当程序写入，后续的读会观察到什么值？
            * 如果是另一个不同的应用读呢？
        * 但是有了复制，每个写必须也发生在其他机器上
        * 很明显，这里有个问题
    * 弱一致性
        * read() 可能返回以前的旧值，不是最近一次写入的结果
    * 强一致性
        * read() 一直返回最近一次的 write()的数据
    * 它们的辩证关系
        * 强一致性对应用写很简单
        * 强一致性对性能影响很大
        * 弱一致性有好的性能并且容易扩展到很多服务器
        * 弱一致性难理解
    * 许多权衡取决于不同的正确性条件
        * 它们被称为“一致性模型”
        * 今天先看看；几乎每篇论文都会出现
* 理想的一致性模型
    * 让我们回到单机情况
    * 如果复制的文件系统表现得像非复制文件系统那样会很好
    * 如果一个应用写入，后续的读会观察到这个写
    * 如果两个应用并发写入相同文件会怎么样呢？
        * 问题：在单机器情况会发生什么？
        * 在文件系统中经常是未定义的 --- 文件可能有一些混合内容
    * 如果两个应用并发写入相同目录会怎么样呢？
        * 问题：在单机器情况会发生什么？
        * 一个先写，另一个后写(用锁)
* 理想一致性的挑战
    * 并发 - 就像我们刚看到的那样；加上现实中有很多硬盘
    * 机器故障 - 任何操作都可能无法完成
    * 网络分区 - 可能无法连接所有的机器/磁盘
    * 为什么这些挑战很难解决？
        * 需要在客户端和服务器之间通信
            * 可能消耗性能
        * 协议变得复杂 - 下周可以看到
            * 很难正确实现系统
        * 很多 6.824 的例子都没有提供理想情况
            * GFS 是一个例子
* GFS 的目标
    * 拥有这么多机器，故障很常见
        * 必须错误容忍
        * 假设一台机器每年发生一次故障
        * 1000 台机器，大约每天故障 3 台(注：1000/365)
    * 高性能：很多并发的读和写
        * Map/Reduce 任务读、存储最终结果在 GFS
        * 注意：不是临时的，中间文件
    * 有效使用网络：节省带宽
    * 这些挑战很难与“理想的”一致性相结合
* 高层次的设计/读
    * Master 存储目录，文件，名字，打开/读、写
        * 但不是 POSIX
    * 数百个有磁盘的 Linux 块服务器
        * 存储 64M 的块(每个块是一个普通的 Linux 文件)
        * 每个块在3个服务器上备份
        * 问题：除了数据的可用性，3倍复制给我们带来了什么？
            * 对热点文件读取的负载均衡
            * 亲和性
        * 问题：为什么不仅仅将文件的一个备份存在 RAID 磁盘阵列上呢？
            * RAID 不常用
            * 希望整个机器的容错；不仅仅是存储设备
        * 问题：为什么块这么大？
            * 分摊开销；减少 master(主服务器)中的状态大小
    * GFS  master(主服务器)知道目录层次
        * 对于目录，知道里面有什么文件
        * 对于文件，知道每个块(64M)的块服务器
        * master(主服务器)将状态保存在内存中
            * 每个块 64 字节的元数据
        * master(主服务器)拥有可用于元数据的私有可恢复数据库
            * 操作日志刷到磁盘
            * 偶尔异步压缩信息检查点
            * master(主服务器)可以很快从断电故障中恢复
        * 影子 master(主服务器)落后master(主服务器)一点
            * 可以升级为 master(主服务器)
    * 客户端读
        * 给 master(主服务器)发送文件名和块索引
        * master(主服务器) 回复一系列拥有这个块的服务器
            * 回复包括块的版本
            * 客户端缓存这些信息
        * 请求最近的块服务器
            * 严重版本号
            * 如果版本号错了，重连 master(主服务器)