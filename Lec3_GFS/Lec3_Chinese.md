6.824 2018 第3课：GFS

The Google File System  
Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung     
SOSP 2003   

* 我们为什么要读这篇论文？
    * 用于 map/reduce 的文件系统
    * 6.284 课程的主要主题都在文中有所展示
        * 简单性和性能的交易一致性
        * 后续设计的动机
    * 很好的系统论文 -- 从应用到网络的详细信息
        * 性能，容错，一致性
    * 有影响力
        * 很多其他的系统使用 GFS(例如 Bigtable, Spanner)
        * HDFS (Hadoop Distributed File System) 基于 GFS
* 什么是一致性
    * 一个正确的条件
    * 在数据是多份时很重要但是很难实现
        * 特别是当应用并发访问的时候
        * 当程序写入，后续的读会观察到什么值？
            * 如果是另一个不同的应用读呢？
        * 但是有了复制，每个写必须也发生在其他机器上
        * 很明显，这里有个问题
    * 弱一致性
        * read() 可能返回以前的旧值，不是最近一次写入的结果
    * 强一致性
        * read() 一直返回最近一次的 write()的数据
    * 它们的辩证关系
        * 强一致性对应用写很简单
        * 强一致性对性能影响很大
        * 弱一致性有好的性能并且容易扩展到很多服务器
        * 弱一致性难理解
    * 许多权衡取决于不同的正确性条件
        * 它们被称为“一致性模型”
        * 今天先看看；几乎每篇论文都会出现
* 理想的一致性模型
    * 让我们回到单机情况
    * 如果复制的文件系统表现得像非复制文件系统那样会很好
    * 如果一个应用写入，后续的读会观察到这个写
    * 如果两个应用并发写入相同文件会怎么样呢？
        * 问题：在单机器情况会发生什么？
        * 在文件系统中经常是未定义的 --- 文件可能有一些混合内容
    * 如果两个应用并发写入相同目录会怎么样呢？
        * 问题：在单机器情况会发生什么？
        * 一个先写，另一个后写(用锁)
* 理想一致性的挑战
    * 并发 - 就像我们刚看到的那样；加上现实中有很多硬盘
    * 机器故障 - 任何操作都可能无法完成
    * 网络分区 - 可能无法连接所有的机器/磁盘
    * 为什么这些挑战很难解决？
        * 需要在客户端和服务器之间通信
            * 可能消耗性能
        * 协议变得复杂 - 下周可以看到
            * 很难正确实现系统
        * 很多 6.824 的例子都没有提供理想情况
            * GFS 是一个例子
* GFS 的目标
    * 拥有这么多机器，故障很常见
        * 必须错误容忍
        * 假设一台机器每年发生一次故障
        * 1000 台机器，大约每天故障 3 台(注：1000/365)
    * 高性能：很多并发的读和写
        * Map/Reduce 任务读、存储最终结果在 GFS
        * 注意：不是临时的，中间文件
    * 有效使用网络：节省带宽
    * 这些挑战很难与“理想的”一致性相结合