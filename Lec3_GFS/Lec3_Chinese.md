6.824 2018 第3课：GFS

The Google File System  
Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung     
SOSP 2003   

* 我们为什么要读这篇论文？
    * 用于 map/reduce 的文件系统
    * 6.284 课程的主要主题都在文中有所展示
        * 简单性和性能的交易一致性
        * 后续设计的动机
    * 很好的系统论文 -- 从应用到网络的详细信息
        * 性能，容错，一致性
    * 有影响力
        * 很多其他的系统使用 GFS(例如 Bigtable, Spanner)
        * HDFS (Hadoop Distributed File System) 基于 GFS
* 什么是一致性
    * 一个正确的条件
    * 在数据是多份时很重要但是很难实现
        * 特别是当应用并发访问的时候
        * 当程序写入，后续的读会观察到什么值？
            * 如果是另一个不同的应用读呢？
        * 但是有了复制，每个写必须也发生在其他机器上
        * 很明显，这里有个问题
    * 弱一致性
        * read() 可能返回以前的旧值，不是最近一次写入的结果
    * 强一致性
        * read() 一直返回最近一次的 write()的数据
    * 它们的辩证关系
        * 强一致性对应用写很简单
        * 强一致性对性能影响很大
        * 弱一致性有好的性能并且容易扩展到很多服务器
        * 弱一致性难理解
    * 许多权衡取决于不同的正确性条件
        * 它们被称为“一致性模型”
        * 今天先看看；几乎每篇论文都会出现
* 理想的一致性模型
    * 让我们回到单机情况
    * 如果复制的文件系统表现得像非复制文件系统那样会很好
    * 如果一个应用写入，后续的读会观察到这个写
    * 如果两个应用并发写入相同文件会怎么样呢？
        * 问题：在单机器情况会发生什么？
        * 在文件系统中经常是未定义的 --- 文件可能有一些混合内容
    * 如果两个应用并发写入相同目录会怎么样呢？
        * 问题：在单机器情况会发生什么？
        * 一个先写，另一个后写(用锁)
* 理想一致性的挑战
    * 并发 - 就像我们刚看到的那样；加上现实中有很多硬盘
    * 机器故障 - 任何操作都可能无法完成
    * 网络分区 - 可能无法连接所有的机器/磁盘
    * 为什么这些挑战很难解决？
        * 需要在客户端和服务器之间通信
            * 可能消耗性能
        * 协议变得复杂 - 下周可以看到
            * 很难正确实现系统
        * 很多 6.824 的例子都没有提供理想情况
            * GFS 是一个例子
* GFS 的目标
    * 拥有这么多机器，故障很常见
        * 必须错误容忍
        * 假设一台机器每年发生一次故障
        * 1000 台机器，大约每天故障 3 台(注：1000/365)
    * 高性能：很多并发的读和写
        * Map/Reduce 任务读、存储最终结果在 GFS
        * 注意：不是临时的，中间文件
    * 有效使用网络：节省带宽
    * 这些挑战很难与“理想的”一致性相结合
* 高层次的设计/读   
    [图1 -- master 和 chunk 服务器]
    * Master 存储目录，文件，名字，打开/读、写
        * 但不是 POSIX
    * 数百个有磁盘的 Linux chunk 服务器
        * 存储 64M 的块(每个块是一个普通的 Linux 文件)
        * 每个块在3个服务器上备份
        * 问题：除了数据的可用性，3倍复制给我们带来了什么？
            * 对热点文件读取的负载均衡
            * 亲和性
        * 问题：为什么不仅仅将文件的一个备份存在 RAID 磁盘阵列上呢？
            * RAID 不常用
            * 希望整个机器的容错；不仅仅是存储设备
        * 问题：为什么块这么大？
            * 分摊开销；减少 master 中的状态大小
    * GFS  master 知道目录层次
        * 对于目录，知道里面有什么文件
        * 对于文件，知道每个块(64M)的 chunk 服务器
        * master 将状态保存在内存中
            * 每个块 64 字节的元数据
        * master 拥有可用于元数据的私有可恢复数据库
            * 操作日志刷到磁盘
            * 偶尔异步压缩信息检查点
            * master 可以很快从断电故障中恢复
        * 影子 master 落后master 一点
            * 可以升级为 master
    * 客户端读
        * 给 master 发送文件名和块索引
        * master回复一系列拥有这个块的服务器
            * 回复包括块的版本
            * 客户端缓存这些信息
        * 请求最近的 chunk 服务器
            * 严重版本号
            * 如果版本号错了，重连 master
* 写    
    [图2 -- 样式图与文件偏移序列]
    * 随机客户端写入已存在文件
        * 客户端向 master 询问块地址以及主 chunk 服务器
        * master 服务器回复 chunk 服务器的地址，版本以及谁是主 chunk 服务器
            * 主 chunk 服务器拥有(或获得)60秒租约
        * 客户端根据网络拓扑计算副本链
        * 客户端发送数据给第一个副本，副本转发给其他机器
            * 使用管道网络，分发负载
        * 副本回复接收回执
            * 主 chunk 服务器分配序列号并写入
            * 然后通知其他副本写入
            * 一旦全部完成，回复客户端
        * 如果此时有另一个并发客户端写入同一位置会怎么样？
            * 客户端2在客户端1之后得到序列号，覆盖数据
            * 现在客户端2重新写入，这次首先得到序列号(客户端1可能慢点)
            * 写入，但是稍后客户端1来写入并且覆盖了      
               => 所有的副本有同样的数据(= 一致)，但是客户端1/2的混合部分(= 未定义)
    * 增加客户端(不是增加记录)
        * 相同的操作，但是可能以任意顺序从客户端1和客户端2写入
        * 一致，但未定义
        * 或者，如果只有一个客户端写，没问题 -- 都一致而且定义了
* 记录追加
    * 客户端记录追加
        * 客户端向 master 询问 chunk 的地址
        * 客户端向副本推送数据，但没有指定偏移量
        * 当数据已推送到所有 chunk 服务器后客户端连接主 chunk 服务器
            * 主 chunk 服务器分配序列号
            * 主 chunk 检查追加是否适合写入 chunk
                * 如果不适合(注：超过最大尺寸 64M)，首先追加直到块边界
            * 主 chunk 挑选追加的偏移量
            * 主 chunk 本地执行变更操作
            * 主 chunk 将请求传送给其他副本
            * 让我们看到 R3 在执行吸入的过程中失败了
            * 主 chunk 检测到失败，通知客户端再试一次
        * 客户端在重连 master 后重试
            * master 可能在此期间带来了R4(或者 R3 回来了)
            * 一个副本现在在字节序列中有一个间隙，所以不能只是追加
            * 用下一个可用偏移量填充所有副本
            * 主 chunk 和二级副本执行写入
            * 主 chunk 在收到所有副本的确认之后回复客户端
* 管理
    * 如果 master 不刷新租约，master 可以指定一个新的主 chunk 服务器
    * master 复制 chunk 服务器如果副本服务器数量低于某个数值
    * master 重新负载均衡副本
* 故障
    * chunk 服务器很容易被替换
        * 故障可能导致一些客户端重试(重复记录)
    * master 宕机导致 GFS 不可用
        * 影子 master 可以提供只读操作，可能会返回旧数据
        * 问题：为什么不提供写操作
            * 脑裂综合征(详见下一课)
* GFS 满足“理想化”一致性吗？
    * 两个案例：目录和文件
    * 目录：是的，但是。。。
        * 是的：强一致性(只有一个拷贝)
        * 但是：master 不一定总是可用，可扩展性限制
    * 文件：不总是
        * 原子性追加的突变
        * 记录可能在两个偏移处重复
        * 当其他副本可能在某个偏移处有间隔
        * 没有原子性追加的突变
            * 数个客户端的数据可能混在一起
            * 如果你介意，使用原子追加或者临时文件和原子性重命名
    * 一个“不幸”的客户端可能在一个很短的时间内读到旧数据
        * 失败的突变导致块不一致
            * 主 chunk 服务器更新了块
            * 但后来失败了，副本过期了
        * 一个客户端可能从一个过期的 chunk 读取数据
        * 当客户端刷新租期会得到新版本号
* 作者声明弱一致性不是应用的大问题
    * 大部分文件的更新是仅仅追加的更新
        * 应用可以在记录追加中使用 UID 来检测重复
        * 应用可能是读取很少数据(但不是旧数据)
    * 应用可以使用临时文件和原子性重命名
* 性能(图3)
    * 读取的巨大聚合吞吐量（3个副本，条带化）
        * 125 MB/秒 在聚合内
        * 接近饱和网络
    * 写入低于可能最大值的不同文件
        * 作者抱怨他们的网络栈
        * 导致从一个副本传播到下一个副本的延迟
    * 并发追加吸入单个文件
        * 受存储最后一个块的服务器的限制
    * 个数和细节在 15 年内改变了好多
* 总结
    * 性能案例学习，容错，一致性
        * 专门用于 MapReduce 应用程序
    * 什么在 GFS 中运行良好
        * 大量的顺序读写
        * 追加
        * 巨大的吞吐量
        * 数据的容错(3 份拷贝)
    * 什么在 GFS 中不是那么好
        * master 容错
        * 小文件(master 瓶颈)
        * 客户端可能看到旧数据
        * 追加可能重复
* 引用
    * http://queue.acm.org/detail.cfm?id=1594206  (gfs 进化讨论)
    * http://highscalability.com/blog/2010/9/11/googles-colossus-makes-search-real-time-by-dumping-mapreduce.html